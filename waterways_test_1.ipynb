{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "from __future__ import print_function\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "landMaskIm = Image.open('land_mask.png').convert('LA')\n",
    "realMapIm = Image.open('clean_map.png').convert('LA')\n",
    "landMaskIm_flip = landMaskIm.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "realMapIm_flip = realMapIm.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "\n",
    "#realMapIm.show()\n",
    "\n",
    "#landMaskIm.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find squares of the land mask that have both water and land"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True True False\n",
      "(1015, 1831)\n",
      "(1015, 1831)\n"
     ]
    }
   ],
   "source": [
    "npLMI = np.array(landMaskIm)[:,:,0]\n",
    "npLMIF = np.array(landMaskIm_flip)[:,:,0]\n",
    "npCMI = np.array(realMapIm)[:,:,0]\n",
    "npCMIF = np.array(realMapIm_flip)[:,:,0]\n",
    "#print(npLMI)\n",
    "print(0 in npLMI, 255 in npLMI, 120 in npLMI)\n",
    "print(npLMI.shape)\n",
    "print(npCMI.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nPics = 10000\n",
    "subSquareSize = (52,52) # Must be multiple of 4!\n",
    "maskSubSquares = np.zeros((nPics, subSquareSize[0], subSquareSize[1]))\n",
    "realSubSquares = np.zeros((nPics, subSquareSize[0], subSquareSize[1]))\n",
    "subSquareCoords = []\n",
    "validSquareIndex = 0\n",
    "while validSquareIndex < nPics:\n",
    "    xCoord = np.random.randint(0,npLMI.shape[0]-subSquareSize[0])\n",
    "    yCoord = np.random.randint(0,npLMI.shape[1]-subSquareSize[1])\n",
    "    #print(xCoord, yCoord)\n",
    "    candidateSubSquare = npLMI[xCoord:xCoord+subSquareSize[0],\n",
    "                               yCoord:yCoord+subSquareSize[1]]\n",
    "    if 50 < np.mean(candidateSubSquare) < 225:\n",
    "        #print(candidateSubSquare.shape)\n",
    "        #print(candidateSubSquare)\n",
    "        subSquareCoords.append((xCoord, yCoord))\n",
    "        maskSubSquare =  npLMI[xCoord:xCoord+subSquareSize[0],\n",
    "                               yCoord:yCoord+subSquareSize[1]]\n",
    "        maskSubSquares[validSquareIndex] = maskSubSquare\n",
    "        \n",
    "        realSubSquare =  npCMI[xCoord:xCoord+subSquareSize[0],\n",
    "                               yCoord:yCoord+subSquareSize[1]]\n",
    "        realSubSquares[validSquareIndex] = realSubSquare\n",
    "\n",
    "        validSquareIndex += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Check results of search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nPicsToShow = 100\n",
    "nCols = 10\n",
    "nRows = 2*(nPicsToShow+1) / (nCols)\n",
    "for i in range(nPicsToShow):\n",
    "    xCoord = subSquareCoords[i][0]\n",
    "    yCoord = subSquareCoords[i][1]\n",
    "    plt.subplot(nRows, nCols, (i*2)+1)\n",
    "    #maskSubSquare = npLMI[xCoord:xCoord+subSquareSize[0],\n",
    "    #                      yCoord:yCoord+subSquareSize[1]]\n",
    "    plt.imshow(maskSubSquares[i])\n",
    "    plt.subplot(nRows, nCols, (i*2)+2)\n",
    "    #realSubSquare = npCMI[xCoord:xCoord+subSquareSize[0],\n",
    "    #                      yCoord:yCoord+subSquareSize[1]]\n",
    "    plt.imshow(realSubSquares[i])\n",
    "#plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Traceback (most recent call last):\n  File \"/home/j5wagner/miniconda2/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\n    from tensorflow.python.pywrap_tensorflow_internal import *\n  File \"/home/j5wagner/miniconda2/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\n    _pywrap_tensorflow_internal = swig_import_helper()\n  File \"/home/j5wagner/miniconda2/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\nImportError: /lib64/libc.so.6: version `GLIBC_2.16' not found (required by /home/j5wagner/miniconda2/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so)\n\n\nFailed to load the native TensorFlow runtime.\n\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\n\nfor some common reasons and solutions.  Include the entire stack trace\nabove this error message when asking for help.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-67e8d853ee25>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAdam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/j5wagner/miniconda2/lib/python2.7/site-packages/keras/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mabsolute_import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mapplications\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/j5wagner/miniconda2/lib/python2.7/site-packages/keras/utils/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdata_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mio_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconv_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Globally-importable utils.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/j5wagner/miniconda2/lib/python2.7/site-packages/keras/utils/conv_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmoves\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbackend\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/j5wagner/miniconda2/lib/python2.7/site-packages/keras/backend/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;32melif\u001b[0m \u001b[0m_BACKEND\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'tensorflow'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Using TensorFlow backend.\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mtensorflow_backend\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Unknown backend: '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_BACKEND\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/j5wagner/miniconda2/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mprint_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmoving_averages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensor_array_ops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/j5wagner/miniconda2/lib/python2.7/site-packages/tensorflow/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# pylint: disable=g-bad-import-order\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpywrap_tensorflow\u001b[0m  \u001b[0;31m# pylint: disable=unused-import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;31m# pylint: disable=wildcard-import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# pylint: disable=redefined-builtin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/j5wagner/miniconda2/lib/python2.7/site-packages/tensorflow/python/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpywrap_tensorflow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;31m# Protocol buffers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/j5wagner/miniconda2/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msome\u001b[0m \u001b[0mcommon\u001b[0m \u001b[0mreasons\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msolutions\u001b[0m\u001b[0;34m.\u001b[0m  \u001b[0mInclude\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mentire\u001b[0m \u001b[0mstack\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m above this error message when asking for help.\"\"\" % traceback.format_exc()\n\u001b[0;32m---> 74\u001b[0;31m   \u001b[0;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;31m# pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: Traceback (most recent call last):\n  File \"/home/j5wagner/miniconda2/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\n    from tensorflow.python.pywrap_tensorflow_internal import *\n  File \"/home/j5wagner/miniconda2/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\n    _pywrap_tensorflow_internal = swig_import_helper()\n  File \"/home/j5wagner/miniconda2/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\nImportError: /lib64/libc.so.6: version `GLIBC_2.16' not found (required by /home/j5wagner/miniconda2/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so)\n\n\nFailed to load the native TensorFlow runtime.\n\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\n\nfor some common reasons and solutions.  Include the entire stack trace\nabove this error message when asking for help."
     ]
    }
   ],
   "source": [
    "from keras.models import Model, Sequential\n",
    "from keras.layers import *\n",
    "from keras.optimizers import Adam\n",
    "import keras\n",
    "from tqdm import tqdm\n",
    "from keras.layers.advanced_activations import LeakyReLU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = maskSubSquares\n",
    "Y_train = realSubSquares\n",
    "\n",
    "# Scaling the range of the image to [-1, 1]\n",
    "# Because we are using tanh as the activation function in the last layer of the generator\n",
    "# and tanh restricts the weights in the range [-1, 1]\n",
    "max_X = np.max(X_train)\n",
    "factor = max_X / 2\n",
    "X_train = (X_train - factor) / factor\n",
    "\n",
    "max_Y = np.max(Y_train)\n",
    "factor = max_X / 2\n",
    "Y_train = (Y_train - factor) / factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Original\n",
    "generator = Sequential([\n",
    "        Dense((subSquareSize[0]/4)*(subSquareSize[1]/4)*16, \n",
    "              input_dim=subSquareSize[0]*subSquareSize[1], \n",
    "              activation=LeakyReLU(0.2)),\n",
    "        BatchNormalization(),\n",
    "        Reshape((subSquareSize[0]/4,subSquareSize[0]/4,16)),\n",
    "        UpSampling2D(),\n",
    "        Convolution2D(8, (5, 5), padding='same', activation=LeakyReLU(0.2)),\n",
    "        BatchNormalization(),\n",
    "        UpSampling2D(),\n",
    "        Convolution2D(1, (5, 5), padding='same', activation='tanh')\n",
    "    ])\n",
    "generator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# One to use for waterways\n",
    "generator = Sequential([\n",
    "        #BatchNormalization(),\n",
    "        Convolution2D(4, (5, 5), input_shape=(subSquareSize[0],subSquareSize[1],1), padding='same', activation=LeakyReLU(0.2)),\n",
    "        BatchNormalization(),\n",
    "        Reshape((subSquareSize[0]*subSquareSize[1]*4,), input_shape=(subSquareSize[0],subSquareSize[1],4)),\n",
    "        Dense((subSquareSize[0]*subSquareSize[1]*4), input_dim=(subSquareSize[0]*subSquareSize[1]*4), activation=LeakyReLU(0.2)),\n",
    "        Reshape((subSquareSize[0],subSquareSize[1],4), input_shape=(subSquareSize[0]*subSquareSize[1]*4,)),\n",
    "        #UpSampling2D(),\n",
    "        Convolution2D(4, (5, 5), padding='same', activation=LeakyReLU(0.2)),\n",
    "        BatchNormalization(),\n",
    "        #UpSampling2D(),\n",
    "        Convolution2D(1, (5, 5), padding='same', activation='tanh'),\n",
    "        #Concatenate()\n",
    "    \n",
    "    ])\n",
    "generator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "discriminator = Sequential([\n",
    "        Convolution2D(64, 5, 5, subsample=(2,2), input_shape=(subSquareSize[0]*2,subSquareSize[1],1), border_mode='same', activation=LeakyReLU(0.2)),\n",
    "        Dropout(0.3),\n",
    "        Convolution2D(128, 5, 5, subsample=(2,2), border_mode='same', activation=LeakyReLU(0.2)),\n",
    "        Dropout(0.3),\n",
    "        Flatten(),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "generator.compile(loss='binary_crossentropy', optimizer=Adam())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "discriminator.compile(loss='binary_crossentropy', optimizer=Adam())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "discriminator.trainable = False\n",
    "ganInput = Input(shape=(subSquareSize[0], subSquareSize[1],1))\n",
    "# getting the output of the generator\n",
    "# and then feeding it to the discriminator\n",
    "# new model = D(G(input))\n",
    "x = generator(ganInput)\n",
    "#catLayer = Sequential()\n",
    "#discInput = Concatenate([x,ganInput], input_shape=(subSquareSize[0]*2, subSquareSize[1]))\n",
    "#catLayer.add(Concatenate([ganInput,ganInput,ganInput], input_shape=(subSquareSize[0]*2, subSquareSize[1])))\n",
    "#catLayer.add(keras.layers.concatenate([x,ganInput], input_shape=(subSquareSize[0]*2, subSquareSize[1])))\n",
    "discInput = keras.layers.concatenate([x,ganInput], \n",
    "                                     axis=1,\n",
    "                                     input_shape=(subSquareSize[0]*2, subSquareSize[1]))\n",
    "#discInput = Input([x,ganInput])\n",
    "ganOutput = discriminator(discInput)\n",
    "#ganOutput = discriminator(x)\n",
    "\n",
    "gan = Model(input=ganInput, output=ganOutput)\n",
    "gan.compile(loss='binary_crossentropy', optimizer=Adam())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gan.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(epoch=10, batch_size=128):\n",
    "    batch_count = X_train.shape[0] // batch_size\n",
    "    \n",
    "    for i in range(epoch):\n",
    "        for j in tqdm(range(batch_count)):\n",
    "            # Input for the generator\n",
    "            #noise_input = np.random.rand(batch_size, 100)\n",
    "            \n",
    "            # getting random images from X_train of size=batch_size \n",
    "            # these are the real images that will be fed to the discriminator\n",
    "            image_batch_indices = np.random.randint(0, X_train.shape[0], size=batch_size)\n",
    "            mask_image_batch = np.zeros((batch_size, subSquareSize[0],subSquareSize[1],1))\n",
    "            for i in range(batch_size):\n",
    "                mask_image_batch[i,:,:,0] = X_train[image_batch_indices[i],:,:]#.flatten()\n",
    "            #mask_image_batch = X_train[image_batch_indices]\n",
    "            #print('mask_image_batch.shape', mask_image_batch.shape)\n",
    "            real_image_batch = np.zeros((batch_size, subSquareSize[0], subSquareSize[1], 1))\n",
    "            real_image_batch[:,:,:,0] = Y_train[image_batch_indices]\n",
    "            \n",
    "            # these are the predicted images from the generator\n",
    "            predictions = generator.predict(mask_image_batch, batch_size=batch_size)#[:,:,:,0]\n",
    "            predictionsAndMasks = np.concatenate([predictions, mask_image_batch], axis=1)\n",
    "            realImagesAndMasks = np.concatenate([real_image_batch, mask_image_batch], axis=1)\n",
    "            # the discriminator takes in the real images and the generated images\n",
    "            #print(predictions.shape, real_image_batch.shape)\n",
    "            #X = np.concatenate([predictions, real_image_batch], axis=0)\n",
    "            #print('X.shape', X.shape)\n",
    "            X = np.concatenate([predictionsAndMasks, realImagesAndMasks], axis=0)\n",
    "            #X = np.concatenate([predictions, real_image_batch], axis=0)\n",
    "            \n",
    "            # labels for the discriminator\n",
    "            y_discriminator = [0]*batch_size + [1]*batch_size\n",
    "            \n",
    "            # Let's train the discriminator\n",
    "            discriminator.trainable = True\n",
    "            loss = discriminator.train_on_batch(X, y_discriminator)\n",
    "            print('epoch %i loss %f' %(epoch, loss))\n",
    "            # Let's train the generator\n",
    "            #noise_input = np.random.rand(batch_size, 100)\n",
    "            image_batch_indices = np.random.randint(0, X_train.shape[0], size=batch_size)\n",
    "            mask_image_batch_2 = np.zeros((batch_size, subSquareSize[0],subSquareSize[1],1))\n",
    "            for i in range(batch_size):\n",
    "                mask_image_batch_2[i,:,:,0] = X_train[image_batch_indices[i],:,:]#.flatten()\n",
    "            discriminator.trainable = False\n",
    "            y_generator = [1]*batch_size\n",
    "            gan.train_on_batch(mask_image_batch_2, y_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train(30, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_output():\n",
    "    nToPlot = 25\n",
    "    #try_input = (np.random.rand(100,subSquareSize[0]*subSquareSize[1]))\n",
    "    #try_input = np.zeros((100,100)) + 1\n",
    "    #try_input[30:70,:] += 1.5\n",
    "    #try_input[70:100,:] += 2.5\n",
    "    #try_input = np.array(try_input)\n",
    "    image_batch_indices = np.random.randint(0, X_train.shape[0], size=nToPlot)\n",
    "    mask_image_batch = np.zeros((nToPlot, subSquareSize[0],subSquareSize[1],1))\n",
    "    for i in range(nToPlot):\n",
    "        mask_image_batch[i,:,:,0] = X_train[image_batch_indices[i],:,:]#.flatten()\n",
    "    preds = generator.predict(mask_image_batch)\n",
    "    predsAndMasks = np.concatenate([mask_image_batch, preds], axis=1)\n",
    "    #preds = generator.predict(np.array([[1]*10,2,2,2,3,3,3]))\n",
    "    plt.clf()\n",
    "    plt.figure(figsize=(10,10))\n",
    "    for i in range(preds.shape[0]):\n",
    "        plt.subplot(5, 5, i+1)\n",
    "        #plt.imshow(preds[i, :, :, 0], cmap='gray')\n",
    "        plt.imshow(predsAndMasks[i, :, :, 0], cmap='gray')\n",
    "        plt.axis('off')\n",
    "    \n",
    "    # tight_layout minimizes the overlap between 2 sub-plots\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Saving 30')\n",
    "generator.save_weights('2018_06_04_waterways_gen_30.h5')\n",
    "discriminator.save_weights('2018_06_04_waterways_dis_30.h5')\n",
    "#plot_output()\n",
    "train(30, 128)\n",
    "print('Saving 60')\n",
    "generator.save_weights('2018_06_04_waterways_gen_60.h5')\n",
    "discriminator.save_weights('2018_06_04_waterways_dis_60.h5')\n",
    "#plot_output()\n",
    "train(30, 128)\n",
    "print('Saving 90')\n",
    "generator.save_weights('2018_06_04_waterways_gen_90.h5')\n",
    "discriminator.save_weights('2018_06_04_waterways_dis_90.h5')\n",
    "train(30, 128)\n",
    "#plot_output()\n",
    "print('Saving 120')\n",
    "generator.save_weights('2018_06_04_waterways_gen_120.h5')\n",
    "discriminator.save_weights('2018_06_04_waterways_dis_120.h5')\n",
    "train(30, 128)\n",
    "#plot_output()\n",
    "print('Saving 150')\n",
    "generator.save_weights('2018_06_04_waterways_gen_150.h5')\n",
    "discriminator.save_weights('2018_06_04_waterways_dis_150.h5')\n",
    "#plot_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "# getting random images from X_train of size=batch_size \n",
    "# these are the real images that will be fed to the discriminator\n",
    "image_batch_indices = np.random.randint(0, X_train.shape[0], size=batch_size)\n",
    "mask_image_batch = np.zeros((batch_size, subSquareSize[0],subSquareSize[1],1))\n",
    "for i in range(batch_size):\n",
    "    mask_image_batch[i,:,:,0] = X_train[image_batch_indices[i],:,:]#.flatten()\n",
    "#mask_image_batch = X_train[image_batch_indices]\n",
    "#print('mask_image_batch.shape', mask_image_batch.shape)\n",
    "real_image_batch = np.zeros((batch_size, subSquareSize[0], subSquareSize[1], 1))\n",
    "real_image_batch[:,:,:,0] = Y_train[image_batch_indices]\n",
    "            \n",
    "# these are the predicted images from the generator\n",
    "predictions = generator.predict(mask_image_batch, batch_size=batch_size)#[:,:,:,0]\n",
    "predictionsAndMasks = np.concatenate([predictions, mask_image_batch], axis=1)\n",
    "realImagesAndMasks = np.concatenate([real_image_batch, mask_image_batch], axis=1)\n",
    "# the discriminator takes in the real images and the generated images\n",
    "#print(predictions.shape, real_image_batch.shape)\n",
    "#X = np.concatenate([predictions, real_image_batch], axis=0)\n",
    "#print('X.shape', X.shape)\n",
    "X = np.concatenate([predictionsAndMasks, realImagesAndMasks], axis=2)\n",
    "#X = np.concatenate([predictions, real_image_batch], axis=0)\n",
    "            \n",
    "# labels for the discriminator\n",
    "y_discriminator = [0]*batch_size + [1]*batch_size\n",
    "            \n",
    "# Let's train the discriminator\n",
    "discriminator.trainable = True\n",
    "result = discriminator.train_on_batch(X, y_discriminator)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# See why this doesn't look so great"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(discriminator.metrics_names)\n",
    "print(result)\n",
    "plt.clf()\n",
    "plt.figure(figsize=(10,10))\n",
    "for i in range(X.shape[0]):\n",
    "    plt.subplot(5, 5, i+1)\n",
    "    #plt.imshow(preds[i, :, :, 0], cmap='gray')\n",
    "    #plt.imshow(predictionsAndMasks[i, :, :, 0], cmap='gray')\n",
    "    plt.imshow(X[i, :, :, 0], cmap='gray')\n",
    "    plt.axis('off')\n",
    "    \n",
    "# tight_layout minimizes the overlap between 2 sub-plots\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I think this means the discriminator is too good"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
